# -*- coding: utf-8 -*-
"""Llama_Chatbot.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1XXEk-qlDo7Ja2271JjZGjVuwlxcwn1qI

***Install Important Libraries***
"""

!pip install transformers torch accelerate
!pip install -q huggingface_hub

"""**Gain access to Hugging face repository to use pretrained models like LLama and mistral**"""

from huggingface_hub import login

login()

from huggingface_hub import whoami
print(whoami()["name"])

from transformers import AutoTokenizer
import transformers
import torch

llama_model = "meta-llama/Llama-2-7b-chat-hf"
tokenizer = AutoTokenizer.from_pretrained(llama_model,use_auth_token=True) # use my huggingface token to use this model to convrt input into embeddings

"""Import the pipeline for predefined task performance"""

from transformers import pipeline
llama_pipeline = pipeline("text-generation",model=llama_model,tokenizer=tokenizer,torch_dtype=torch.float16,device_map="auto")

"""Pipeline is setup, Now we need to create a chatbot using llama"""

#Create a dictonary to hold reponses and add a system role for model clarity

messages = [{
    "role":"system",
    "content":"You are a helpful, smart and friendly AI assistant. Do not leave the sentence incomplete."
}]

# create a function to get llama response

def get_llama_response(user_input: str) -> str: # Takes the user input and return a reply
  messages.append({"role":"user","content":user_input}) # save user input to dictionary

  #Convert chat into Llama format for chat models

  prompt = tokenizer.apply_chat_template(
      messages,
      tokenize=False,# telling llama not to give output as tokens, intead words
      add_generation_prompt=True # tells the model its assistant turn to reply
  )

  # generate response

  output = llama_pipeline(prompt,max_new_tokens=256,do_sample=True,temperature=0.7, top_k=5, top_p=0.9, eos_token_id = tokenizer.eos_token_id)

  full_text = output[0]["generated_text"]
  reply = full_text[len(prompt):].strip() # remove prompt from response
  # save assistant reply to memory
  messages.append({"role":"assistant","content":reply})
  return reply

# CLI loop
def run_chatbot():
  print("Llama chatbot is ready")

  while True:
    user_input = input("user: ")

    if user_input.lower() == "exit":
      print("chat ended.")
      break

    reply = get_llama_response(user_input)

    print("Chatbot: ",reply)

if __name__ == "__main__":
  run_chatbot()