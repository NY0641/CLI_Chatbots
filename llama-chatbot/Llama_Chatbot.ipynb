{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### **üîß Environment Setup**\n",
        "\n",
        "*  Install required libraries once at runtime.\n",
        "*  These are intentionally separated from model logic\n",
        "* to keep dependency management clear and reproducible.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "g7TGYAmamUSr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M9WxWBUB-hu0"
      },
      "outputs": [],
      "source": [
        "!pip install transformers torch accelerate\n",
        "!pip install -q huggingface_hub\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **üîê Secure HuggingFace Authentication**\n",
        "*   Token is requested at runtime instead of hardcoding it.\n",
        "*   This prevents credential leakage when sharing notebooks\n",
        "* and follows production security practices.\n",
        "\n"
      ],
      "metadata": {
        "id": "fKF3g5TlIwxq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from huggingface_hub import login, whoami\n",
        "\n",
        "def authenticate_huggingface(allow_interactive=True):\n",
        "\n",
        "    token = None\n",
        "\n",
        "    # ------------------------------------------------------\n",
        "    # 1Ô∏è‚É£ ENV VARIABLE (best practice)\n",
        "    # ------------------------------------------------------\n",
        "    token = os.getenv(\"HF_TOKEN\")\n",
        "\n",
        "    # ------------------------------------------------------\n",
        "    # 2Ô∏è‚É£ GOOGLE COLAB SECRETS\n",
        "    # ------------------------------------------------------\n",
        "    if not token:\n",
        "        try:\n",
        "            from google.colab import userdata\n",
        "            token = userdata.get(\"HF_TOKEN\")\n",
        "\n",
        "            # ‚≠ê IMPORTANT FIX: handle dict return\n",
        "            if isinstance(token, dict):\n",
        "                token = token.get(\"value\")\n",
        "\n",
        "            if token:\n",
        "                print(\"üîê Using HuggingFace token from Colab Secrets\")\n",
        "\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "    # ------------------------------------------------------\n",
        "    # 3Ô∏è‚É£ INTERACTIVE FALLBACK\n",
        "    # ------------------------------------------------------\n",
        "    if not token and allow_interactive:\n",
        "        try:\n",
        "            from getpass import getpass\n",
        "            token = getpass(\"Enter HuggingFace token (hidden input): \")\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "    # ------------------------------------------------------\n",
        "    # 4Ô∏è‚É£ LOGIN FLOW\n",
        "    # ------------------------------------------------------\n",
        "    if token:\n",
        "        try:\n",
        "            # ‚≠ê Skip login if session already authenticated\n",
        "            try:\n",
        "                whoami()\n",
        "                print(\"‚úÖ Already authenticated with HuggingFace\")\n",
        "            except Exception:\n",
        "                login(token)\n",
        "\n",
        "            # Confirm user\n",
        "            try:\n",
        "                username = whoami()[\"name\"]\n",
        "                print(f\"‚úÖ HuggingFace login successful ‚Üí {username}\")\n",
        "            except Exception:\n",
        "                print(\"‚úÖ HuggingFace login successful\")\n",
        "\n",
        "        except Exception as e:\n",
        "            raise RuntimeError(\n",
        "                \"‚ùå HuggingFace authentication failed.\\n\"\n",
        "                \"Check that your token is valid and has model access.\\n\"\n",
        "                f\"Error: {str(e)}\"\n",
        "            )\n",
        "\n",
        "    else:\n",
        "        print(\n",
        "            \"‚ö†Ô∏è No HuggingFace token detected.\\n\"\n",
        "            \"If the model is gated, authentication is required.\\n\\n\"\n",
        "            \"Fix:\\n\"\n",
        "            \"‚Ä¢ Set env var: HF_TOKEN=your_token\\n\"\n",
        "            \"‚Ä¢ OR add token in Colab Secrets\\n\"\n",
        "        )\n",
        "\n",
        "\n",
        "# Call before model loading\n",
        "authenticate_huggingface()\n"
      ],
      "metadata": {
        "id": "R2Q1JX2pIXgQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **üñ•Ô∏è Hardware Check (GPU + RAM)**\n",
        "*This cell defines helper functions used during model initialization.\n",
        "\n",
        "- `get_device_config()` decides whether the model runs on GPU or CPU.\n",
        "- `safe_model_load()` prevents the notebook from hanging if model loading fails or takes too long.\n",
        "\n",
        "These utilities keep the loading logic clean"
      ],
      "metadata": {
        "id": "e7K8b4V9nJEj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "def resolve_runtime_config(min_vram_gb=10):\n",
        "    \"\"\"\n",
        "    Decide device placement and precision automatically.\n",
        "\n",
        "    Strategy:\n",
        "    - Use GPU if available AND VRAM sufficient\n",
        "    - Otherwise fall back to CPU\n",
        "    - Never require manual setup\n",
        "\n",
        "    Returns:\n",
        "        torch_dtype, device_map\n",
        "    \"\"\"\n",
        "\n",
        "    # -------------------------\n",
        "    # GPU available?\n",
        "    # -------------------------\n",
        "    if torch.cuda.is_available():\n",
        "\n",
        "        total_vram = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
        "\n",
        "        # GPU exists but too small ‚Üí fallback to CPU\n",
        "        if total_vram < min_vram_gb:\n",
        "            return torch.float32, \"cpu\"\n",
        "\n",
        "        # GPU sufficient ‚Üí use it\n",
        "        return torch.float16, \"auto\"\n",
        "\n",
        "    # -------------------------\n",
        "    # No GPU ‚Üí CPU fallback\n",
        "    # -------------------------\n",
        "    return torch.float32, \"cpu\"\n"
      ],
      "metadata": {
        "id": "5jg4QRrs7M3W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ‚è±Ô∏è Safe Model Loader\n",
        "\n",
        "Loads the model with timeout protection.\n",
        "Prevents notebook from hanging if resources are insufficient.\n"
      ],
      "metadata": {
        "id": "q5nsvFzu8Wvi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "def safe_model_load(load_fn, max_load_time=300):\n",
        "    \"\"\"\n",
        "    Runs model loading with timeout protection.\n",
        "\n",
        "    Raises:\n",
        "        RuntimeError if loading fails or takes too long.\n",
        "    \"\"\"\n",
        "\n",
        "    start = time.time()\n",
        "\n",
        "    try:\n",
        "        result = load_fn()\n",
        "\n",
        "        elapsed = time.time() - start\n",
        "        if elapsed > max_load_time:\n",
        "            raise RuntimeError(\n",
        "                \"Model loading took too long. \"\n",
        "                \"You may be running on CPU or low resources.\"\n",
        "            )\n",
        "\n",
        "        return result\n",
        "\n",
        "    except Exception as e:\n",
        "        raise RuntimeError(f\"Model failed to load: {str(e)}\")\n"
      ],
      "metadata": {
        "id": "Df7hiuDs8bQD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Model Intialization**"
      ],
      "metadata": {
        "id": "CM599tYKnEzg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, pipeline\n",
        "\n",
        "MODEL_NAME = \"meta-llama/Llama-2-7b-chat-hf\"\n",
        "\n",
        "# üîπ Resolve runtime automatically\n",
        "torch_dtype, device_map = resolve_runtime_config()\n",
        "\n",
        "# üîπ Define loading function\n",
        "def load_pipeline():\n",
        "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "\n",
        "    pipe = pipeline(\n",
        "        \"text-generation\",\n",
        "        model=MODEL_NAME,\n",
        "        tokenizer=tokenizer,\n",
        "        torch_dtype=torch_dtype,\n",
        "        device_map=device_map\n",
        "    )\n",
        "\n",
        "    return tokenizer, pipe\n",
        "\n",
        "# üîπ Run with timeout protection\n",
        "tokenizer, llama_pipeline = safe_model_load(load_pipeline)\n"
      ],
      "metadata": {
        "id": "dCGYeadH3iZ3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pipeline is setup, Now we need to create a chatbot using llama"
      ],
      "metadata": {
        "id": "DeszZwDmcU1k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_INPUT_TOKENS = 3000   # safe budget for prompt\n",
        "\n",
        "def count_tokens(messages):\n",
        "    \"\"\"\n",
        "    Convert conversation to tokenized prompt and measure length.\n",
        "    \"\"\"\n",
        "    tokens = tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        tokenize=True,\n",
        "        add_generation_prompt=True,\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "    return tokens[\"input_ids\"].shape[-1]\n"
      ],
      "metadata": {
        "id": "8PILwPpJfJYn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def truncate_history(messages):\n",
        "    \"\"\"\n",
        "    Remove oldest user/assistant messages until prompt fits token budget.\n",
        "    Always keeps the system prompt.\n",
        "    \"\"\"\n",
        "    while count_tokens(messages) > MAX_INPUT_TOKENS and len(messages) > 1:\n",
        "        messages.pop(1)   # remove oldest non-system message\n",
        "    return messages\n"
      ],
      "metadata": {
        "id": "mt7dc3YtBXw_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "messages = [{\n",
        "    \"role\":\"system\",\n",
        "    \"content\":\"You are a helpful, smart and friendly AI assistant. Do not leave the sentence incomplete.\"\n",
        "}]\n",
        "\n",
        "def get_llama_response(user_input: str) -> str:\n",
        "    global messages\n",
        "\n",
        "    # Add user message\n",
        "    messages.append({\"role\":\"user\",\"content\":user_input})\n",
        "\n",
        "    # üîπ Token-based truncation happens here\n",
        "    messages = truncate_history(messages)\n",
        "\n",
        "    # Convert chat into model prompt\n",
        "    prompt = tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        tokenize=False,# telling llama not to give output as tokens, intead words\n",
        "        add_generation_prompt=True # tells the model its assistant turn to reply\n",
        "    )\n",
        "\n",
        "    # Generate response\n",
        "    output = llama_pipeline(\n",
        "        prompt,\n",
        "        max_new_tokens=256,\n",
        "        do_sample=True,\n",
        "        temperature=0.7,\n",
        "        top_k=5,\n",
        "        top_p=0.9,\n",
        "        eos_token_id=tokenizer.eos_token_id\n",
        "    )\n",
        "\n",
        "    full_text = output[0][\"generated_text\"]\n",
        "    reply = full_text[len(prompt):].strip()\n",
        "\n",
        "    # Save assistant reply\n",
        "    messages.append({\"role\":\"assistant\",\"content\":reply})\n",
        "\n",
        "    return reply\n"
      ],
      "metadata": {
        "id": "BwQYh5_jBc9g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CLI loop\n",
        "def run_chatbot():\n",
        "  print(\"Llama chatbot is ready\")\n",
        "\n",
        "  while True:\n",
        "    user_input = input(\"user: \")\n",
        "\n",
        "    if user_input.lower() == \"exit\":\n",
        "      print(\"chat ended.\")\n",
        "      break\n",
        "\n",
        "    reply = get_llama_response(user_input)\n",
        "\n",
        "    print(\"Chatbot: \",reply)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  run_chatbot()"
      ],
      "metadata": {
        "id": "98yV_5LppSln"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}