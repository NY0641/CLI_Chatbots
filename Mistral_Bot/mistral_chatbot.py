# -*- coding: utf-8 -*-
"""Mistral_Chatbot.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1H9ndg1JdKpEl9-PT6-GIqCcD-z3HtPU0
"""

# install Important libraries

!pip install --upgrade transformers # load models from existing library
!pip install --upgrade accelerate # optimized training and inference
!pip install --upgrade bitsandbytes # loads large models in low precision to save GPU memory

from huggingface_hub import login
login()

#bitsandbytes config to control howthe model is quantized to reduce GPu  memory usage

from transformers import BitsAndBytesConfig, AutoTokenizer, AutoModelForCausalLM
import torch
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,                #enable 4bit quantization
    bnb_4bit_use_double_quant=True,   # improves accuracy slightly
    bnb_4bit_quant_type="nf4",        #best quant type for LLMs
    bnb_4bit_compute_dtype=torch.bfloat16 #Computation precision
)

!rm -rf /opt/bin/.nvidia/*
!rm -rf /root/.cache/huggingface
!rm -rf /root/.cache/torch
!pip cache purge

# You select the model → load its tokenizer → load the compressed model onto GPU ready for text generation.
model  = "mistralai/Mistral-7B-Instruct-v0.1"
tokenizer = AutoTokenizer.from_pretrained(model)
mistral_model = AutoModelForCausalLM.from_pretrained(model, quantization_config=bnb_config,torch_dtype=torch.bfloat16,device_map="auto")

# Pipeline = one-step tool that handles tokenizer + model + generation automatically
from transformers import pipeline
mistral_pipeline = pipeline(
    "text-generation",
    model=mistral_model,
    tokenizer=tokenizer,
    device_map="auto",
    torch_dtype=torch.bfloat16
)

def mistral_chat(user_input, messages=None):
  # Initialize messages if it's the first turn, otherwise use the existing history
  if messages is None:
    messages=[
        {"role": "system", "content": "You are a helpful, smart and friendly AI assistant. Do not leave the sentence incomplete."}
    ]

  # Save user input
  messages.append({"role": "user", "content": user_input})

  #Build prompt from memory since prompts for llama and mistral needs to be formated
  prompt = tokenizer.apply_chat_template(
      messages,
      tokenize=False, # telling llama not to give output as tokens, intead words
      add_generation_prompt=True) # tells the model its assistant turn to reply

  #generate_response
  output= mistral_pipeline(prompt,do_sample=True, max_new_tokens=200, temperature=0.1,top_p=0.95)
  full_text = output[0]["generated_text"]
  reply = full_text[len(prompt):].strip() # remove prompt from response
  # save assistant reply to memory
  messages.append({"role":"assistant","content":reply})
  return reply, messages

# Chat Loop Function

def run_bot():
  messages =None
  while True:
    user_input = input("User: ")
    if user_input.lower() == "exit":
      print("Chat ended. Bye!")
      break
    reply, messages = mistral_chat(user_input, messages)
    print(f"Assistant: {reply}")

#main Function
if __name__ == "__main__":
  run_bot()