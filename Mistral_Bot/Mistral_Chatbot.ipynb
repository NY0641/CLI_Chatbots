{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## **Install dependencies**"
      ],
      "metadata": {
        "id": "Gj8Xd38C3Vxt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q transformers accelerate bitsandbytes huggingface_hub"
      ],
      "metadata": {
        "id": "FSMDaM9d3MuI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Secure HuggingFace authentication**"
      ],
      "metadata": {
        "id": "-XcnkNms32Tf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from huggingface_hub import login, whoami\n",
        "\n",
        "def authenticate_huggingface(allow_interactive=True):\n",
        "\n",
        "    token = os.getenv(\"HF_TOKEN\")\n",
        "\n",
        "    # Colab secrets fallback\n",
        "    if not token:\n",
        "        try:\n",
        "            from google.colab import userdata\n",
        "            token = userdata.get(\"HF_TOKEN\")\n",
        "            if isinstance(token, dict):\n",
        "                token = token.get(\"value\")\n",
        "            if token:\n",
        "                print(\"ðŸ” Using token from Colab Secrets\")\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "    # Interactive fallback\n",
        "    if not token and allow_interactive:\n",
        "        from getpass import getpass\n",
        "        token = getpass(\"Enter HuggingFace token: \")\n",
        "\n",
        "    if token:\n",
        "        try:\n",
        "            try:\n",
        "                whoami()\n",
        "                print(\"âœ… Already authenticated\")\n",
        "            except:\n",
        "                login(token)\n",
        "\n",
        "            print(f\"âœ… Logged in as {whoami()['name']}\")\n",
        "        except Exception as e:\n",
        "            raise RuntimeError(f\"Authentication failed: {e}\")\n",
        "    else:\n",
        "        print(\"âš ï¸ No HuggingFace token detected\")\n",
        "\n",
        "authenticate_huggingface()\n"
      ],
      "metadata": {
        "id": "3sxRjMzN4ElF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Runtime hardware detection** - Clean hardware selector with safe fallback"
      ],
      "metadata": {
        "id": "NTjWECNn64cP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import warnings\n",
        "\n",
        "\n",
        "def resolve_runtime_config(min_vram_gb: int = 8):\n",
        "    \"\"\"\n",
        "    Select best runtime device.\n",
        "\n",
        "    - Uses GPU if available and large enough\n",
        "    - Falls back to CPU otherwise\n",
        "    - Warns user if CPU is used (slower inference)\n",
        "\n",
        "    Returns:\n",
        "        (torch_dtype, device_map)\n",
        "    \"\"\"\n",
        "\n",
        "    # --- If GPU exists, check memory ---\n",
        "    if torch.cuda.is_available():\n",
        "        vram_gb = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
        "\n",
        "        if vram_gb >= min_vram_gb:\n",
        "            dtype = (\n",
        "                torch.bfloat16\n",
        "                if torch.cuda.is_bf16_supported()\n",
        "                else torch.float16\n",
        "            )\n",
        "            return dtype, \"auto\"\n",
        "\n",
        "        # GPU exists but too small â†’ fall back\n",
        "        warnings.warn(\n",
        "            f\"GPU detected but only {vram_gb:.1f}GB VRAM available. \"\n",
        "            \"Falling back to CPU. Inference may be slow.\",\n",
        "            RuntimeWarning,\n",
        "        )\n",
        "\n",
        "    else:\n",
        "        warnings.warn(\n",
        "            \"No GPU detected. Running on CPU. Inference may be slow.\",\n",
        "            RuntimeWarning,\n",
        "        )\n",
        "\n",
        "    # --- CPU fallback ---\n",
        "    return torch.float32, \"cpu\"\n",
        "\n",
        "\n",
        "# usage\n",
        "torch_dtype, device_map = resolve_runtime_config()\n"
      ],
      "metadata": {
        "id": "R8jp-TeD7C7e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Safe Model Load Wrapper**"
      ],
      "metadata": {
        "id": "KNu1bJq_76Zr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import time\n",
        "\n",
        "\n",
        "def safe_model_load(load_fn, max_load_time: int = 300):\n",
        "    \"\"\"\n",
        "    Executes model loading with runtime safeguards.\n",
        "\n",
        "    Args:\n",
        "        load_fn: Callable responsible for loading the model.\n",
        "        max_load_time: Maximum allowed load duration (seconds).\n",
        "\n",
        "    Returns:\n",
        "        Loaded model object.\n",
        "\n",
        "    Raises:\n",
        "        RuntimeError: If loading fails or exceeds time limit.\n",
        "    \"\"\"\n",
        "\n",
        "    start_time = time.monotonic()\n",
        "\n",
        "    try:\n",
        "        model = load_fn()\n",
        "\n",
        "    except Exception as exc:\n",
        "        raise RuntimeError(\"Model initialization failed.\") from exc\n",
        "\n",
        "    elapsed = time.monotonic() - start_time\n",
        "\n",
        "    if elapsed > max_load_time:\n",
        "        raise RuntimeError(\n",
        "            f\"Model initialization exceeded {max_load_time}s \"\n",
        "            f\"(actual: {elapsed:.1f}s).\"\n",
        "        )\n",
        "\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "vpBOpfRS76Bq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Load Mistral model (4-bit quantized)**"
      ],
      "metadata": {
        "id": "R_BFDVQw-xm3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BitsAndBytesConfig, AutoTokenizer, AutoModelForCausalLM, pipeline\n",
        "\n",
        "MODEL_NAME=\"mistralai/Mistral-7B-Instruct-v0.1\"\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch_dtype\n",
        ")\n",
        "\n",
        "def load_pipeline():\n",
        "\n",
        "    tokenizer=AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "\n",
        "    model=AutoModelForCausalLM.from_pretrained(\n",
        "        MODEL_NAME,\n",
        "        quantization_config=bnb_config,\n",
        "        torch_dtype=torch_dtype,\n",
        "        device_map=device_map\n",
        "    )\n",
        "\n",
        "    pipe=pipeline(\n",
        "        \"text-generation\",\n",
        "        model=model,\n",
        "        tokenizer=tokenizer,\n",
        "        device_map=device_map,\n",
        "        torch_dtype=torch_dtype\n",
        "    )\n",
        "\n",
        "    return tokenizer, pipe\n",
        "\n",
        "tokenizer, mistral_pipeline = safe_model_load(load_pipeline)\n"
      ],
      "metadata": {
        "id": "nXgrVZ_wCBcv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Token memory management**  \n",
        "\n",
        "\n",
        "##### Memory management with Memory truncation on tokens\n"
      ],
      "metadata": {
        "id": "0oiPvNBq_M8N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_INPUT_TOKENS=6000\n",
        "\n",
        "def count_tokens(messages):\n",
        "\n",
        "    tokens=tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        tokenize=True,\n",
        "        add_generation_prompt=True,\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "    return tokens[\"input_ids\"].shape[-1]\n",
        "\n",
        "def truncate_history(messages):\n",
        "\n",
        "    while count_tokens(messages) > MAX_INPUT_TOKENS and len(messages)>1:\n",
        "        messages.pop(1)\n",
        "\n",
        "    return messages\n"
      ],
      "metadata": {
        "id": "ce8iKozWAzz7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Chat Loop Function\n",
        "\n",
        "messages=[{\n",
        "    \"role\":\"system\",\n",
        "    \"content\":\"You are a helpful, smart and friendly AI assistant. Do not leave the sentence incomplete.\"\n",
        "}]\n",
        "\n",
        "def get_mistral_response(user_input:str)->str:\n",
        "    global messages\n",
        "\n",
        "    messages.append({\"role\":\"user\",\"content\":user_input})\n",
        "\n",
        "    # token-based truncation\n",
        "    messages=truncate_history(messages)\n",
        "\n",
        "    prompt=tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        tokenize=False,\n",
        "        add_generation_prompt=True\n",
        "    )\n",
        "\n",
        "    output=mistral_pipeline(\n",
        "        prompt,\n",
        "        max_new_tokens=256,\n",
        "        do_sample=True,\n",
        "        temperature=0.2,\n",
        "        top_p=0.9\n",
        "    )\n",
        "\n",
        "    full_text=output[0][\"generated_text\"]\n",
        "    reply=full_text[len(prompt):].strip()\n",
        "\n",
        "    messages.append({\"role\":\"assistant\",\"content\":reply})\n",
        "\n",
        "    return reply\n",
        "\n"
      ],
      "metadata": {
        "id": "-p6debDjffTw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_chatbot():\n",
        "\n",
        "    print(\"Mistral chatbot is ready\")\n",
        "\n",
        "    while True:\n",
        "        user_input=input(\"user: \")\n",
        "\n",
        "        if user_input.lower()==\"exit\":\n",
        "            print(\"chat ended.\")\n",
        "            break\n",
        "\n",
        "        reply=get_mistral_response(user_input)\n",
        "        print(\"Chatbot:\",reply)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  run_chatbot()\n",
        "\n"
      ],
      "metadata": {
        "id": "E4P4xcDxAIAc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}